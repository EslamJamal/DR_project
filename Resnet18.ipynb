{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print ('-'*10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            num_correct = 0\n",
    "            \n",
    "                \n",
    "            for data, labels in dataloaders[phase]:\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, labels)\n",
    "                    _, preds = torch.max(output.data, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                running_loss += loss.item()*data.size(0)\n",
    "                num_correct += torch.sum(preds == labels.data)\n",
    "                del data, labels, output, preds\n",
    "                torch.cuda.empty_cache()\n",
    "                #break\n",
    "            epoch_loss =  running_loss / dataSizes[phase]\n",
    "            epoch_acc = num_correct.double() / dataSizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "                val_acc.append(epoch_acc)\n",
    "                \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) \n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            #break    \n",
    "        print()\n",
    "        print('='*70) \n",
    "        print()\n",
    "        #break\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}mins {:.0f}secs'.format(time_elapsed // 60, time_elapsed % 60)) \n",
    "    print('Best accuracy {:.4f}'.format(best_acc * 100))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts) #load it again to return it at the main function\n",
    "    #model.save_state_dict('model_name.pt')\n",
    "    \n",
    "    return model, train_losses, train_acc, val_losses, val_acc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_data = datasets.ImageFolder(root='/media/eslam/7AE0CA83E0CA455B/Users/Eslam100/Documents/kaggledataset/fake_dataset/train/')\\nval_data = datasets.ImageFolder(root='/media/eslam/7AE0CA83E0CA455B/Users/Eslam100/Documents/kaggledataset/fake_dataset/val/')\\ntest_data = datasets.ImageFolder(root='/media/eslam/7AE0CA83E0CA455B/Users/Eslam100/Documents/kaggledataset/fake_dataset/test/')\\n\\nbatch_size = 16\\nlr = 0.001\\n\\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        #transforms.Resize((256, 256)),\n",
    "        transforms.RandomRotation((90, 180)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = '/media/eslam/7AE0CA83E0CA455B/Users/Eslam100/Documents/kaggledataset/new_dataset/'\n",
    "image_dataset = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                 for x in ['train', 'val', 'test']}\n",
    "\n",
    "dataloaders = {x: DataLoader(image_dataset[x],\n",
    "                             batch_size=4,\n",
    "                             shuffle=True,\n",
    "                            num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataSizes = {x: len(image_dataset[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_dataset['train'].classes\n",
    "\n",
    "'''train_data = datasets.ImageFolder(root='/media/eslam/7AE0CA83E0CA455B/Users/Eslam100/Documents/kaggledataset/fake_dataset/train/')\n",
    "val_data = datasets.ImageFolder(root='/media/eslam/7AE0CA83E0CA455B/Users/Eslam100/Documents/kaggledataset/fake_dataset/val/')\n",
    "test_data = datasets.ImageFolder(root='/media/eslam/7AE0CA83E0CA455B/Users/Eslam100/Documents/kaggledataset/fake_dataset/test/')\n",
    "\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    test_losses = []\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    for data, labels in dataloaders['test']:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        _,preds = torch.max(output.data, 1)\n",
    "        running_loss += loss.data[0]*data.size(0)\n",
    "        num_correct += torch.sum(preds == labels.data)\n",
    "        true.extend(labels.data.cpu().numpy().tolist())\n",
    "        pred.extend(preds.cpu().numpy().tolist())\n",
    "        \n",
    "    accuracy = num_correct/dataSizes['test']\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "def plot_conf_matrix(y_test, y_pred, class_names):\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('CCR = {}'.format(np.trace(cnf_matrix) / len(y_test)))\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main (Finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is downloaded successfully\n",
      "Epoch 0/29\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e5f8511a5454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmymodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \"\"\"\n",
      "\u001b[0;32m<ipython-input-3-fdb9e2af87cc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight.data)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight.data)\n",
    "        m.bias.data.normal_(mean=0,std=1e-2)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.uniform_()\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = models.vgg16_bn(pretrained=True)\n",
    "print('model is downloaded successfully')\n",
    "for param in model.features.parameters():\n",
    "    param.require_grad = False\n",
    "    \n",
    "num_ftrs = model.classifier[6].in_features\n",
    "features = list(model.classifier.children())[:-1]\n",
    "features.extend([nn.Linear(num_ftrs, 2)])\n",
    "model.classifier = nn.Sequential(*features)\n",
    "model = model.to(device)\n",
    "\n",
    "\"\"\"\n",
    "model = models.resnet34(pretrained=True)\n",
    "print('model is downloaded successfully')\n",
    "#model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "model.avgpool = nn.AvgPool2d(10,10)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "model = torch.nn.DataParallel(model, device_ids=[0])\n",
    "model.module.fc.apply(weights_init)\n",
    "\"\"\"\n",
    "\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "class_names = ['normal','upnormal']\n",
    "true = []\n",
    "pred = []\n",
    "accuracy =0\n",
    "num_epochs = 30\n",
    "\n",
    "mymodel, train_losses, train_acc, val_losses, val_acc=train_model(model, criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "\"\"\"\n",
    "accuracy = test_model(mymodel, criterion, optimizer, scheduler, num_epochs = num_epochs)\n",
    "plot_conf_matrix(true, pred, class_names)\n",
    "\n",
    "### plotting ###\n",
    "epochsRange = np.array(range(num_epochs))\n",
    "plt.plot(epochsRange, train_losses, label='Training')\n",
    "plt.plot(epochsRange, val_losses, label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochsRange, train_acc, label='Training')\n",
    "plt.plot(epochsRange, val_acc, label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main (Convnet as feature extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "print('model is downloaded successfully')\n",
    "model_conv.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "lr = 0.0005\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_conv.fc.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "class_names = ['normal','upnormal']\n",
    "true = []\n",
    "pred = []\n",
    "accuracy =0\n",
    "num_epochs = 30\n",
    "\n",
    "mymodel, train_losses, train_acc, val_losses, val_acc=train_model(model_conv, criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "accuracy = test_model(mymodel, criterion, optimizer, scheduler, num_epochs = num_epochs)\n",
    "plot_conf_matrix(true, pred, class_names)\n",
    "\n",
    "### plotting ###\n",
    "epochsRange = np.array(range(num_epochs))\n",
    "plt.plot(epochsRange, train_losses, label='Training')\n",
    "plt.plot(epochsRange, val_losses, label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochsRange, train_acc, label='Training')\n",
    "plt.plot(epochsRange, val_acc, label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    test_losses = []\n",
    "    running_loss = 0.0\n",
    "    accuracy = 0\n",
    "    for data, labels in dataloaders['test']:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        _,preds = torch.max(output.data, 1)\n",
    "        running_loss += loss.data[0]*data.size(0)\n",
    "        num_correct += torch.sum(preds == labels.data)\n",
    "        true.extend(labels.data.cpu().numpy().tolist())\n",
    "        pred.extend(preds.cpu().numpy().tolist())\n",
    "        \n",
    "    accuracy = num_correct/dataSizes['test']\n",
    "    return accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
